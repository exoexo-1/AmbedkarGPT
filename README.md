# ğŸ“˜ AmbedkarGPT

### Semantic Graph Retrieval-Augmented Question Answering over Ambedkarâ€™s Writings

AmbedkarGPT is a **Semantic Graph-based Retrieval Augmented Generation (SemRAG)** system built on the writings of **Dr. B. R. Ambedkar**.
It answers conceptual and analytical questions by combining **knowledge graphs**, **community-aware retrieval**, and **local LLM inference**, ensuring **faithful and explainable answers grounded strictly in the source text**.

---

## âœ¨ Key Features

* ğŸ“š **Book-Grounded QA** â€“ Answers are generated exclusively from Ambedkarâ€™s writings
* ğŸ§  **Semantic Graph RAG (SemRAG)** â€“ Entity-level and community-level retrieval
* ğŸ”— **Knowledge Graph Construction** â€“ Entities, relations, and co-occurrence edges
* ğŸŒ **Dual Retrieval Strategy**

  * **Local Graph RAG** (fine-grained evidence)
  * **Global Graph RAG** (thematic context)
* ğŸ¤– **Local LLM Inference** using Ollama (no external APIs)
* ğŸ–¥ï¸ **Gradio-based Q&A Interface** (not a chatbot)

---

## ğŸ“ Project Structure (Simplified)

```
AmbedkarGPT/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pages/              # Page-wise book text
â”‚   â”œâ”€â”€ processed/          # Chunks and embeddings
â”‚   â”œâ”€â”€ graph/              # Knowledge graphs & communities
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ chunking/           # Semantic chunking logic
â”‚   â”œâ”€â”€ graph/              # Graph construction & community detection
â”‚   â”œâ”€â”€ retrieval/          # Local & Global Graph RAG
â”‚   â””â”€â”€ pipeline/
â”‚       â””â”€â”€ AmbedkarGPT.py  # End-to-end demo (run this)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone <private-repo-url>
cd AmbedkarGPT
```

---

### 2. Create & Activate Virtual Environment

```bash
python -m venv venv
```

**Windows**

```bash
venv\Scripts\activate
```

**Linux / macOS**

```bash
source venv/bin/activate
```

---

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 4. Install Ollama (LLM Backend)

AmbedkarGPT uses **Ollama** for local LLM inference.

Download: [https://ollama.com](https://ollama.com)

Pull a model:

```bash
ollama pull mistral
# or
ollama pull llama3
```

Start Ollama:

```bash
ollama serve
```

---

## â–¶ï¸ Running the Demo (Interview-Ready)

Run the complete end-to-end system:

```bash
python src/pipeline/AmbedkarGPT.py
```

This launches a **Gradio Questionâ€“Answer interface** in your browser.

### Input

* Conceptual or analytical questions related to Ambedkarâ€™s writings

  * Example: *â€œExplain caste in relation to religion and societyâ€*

### Output

* âœ… Final generated answer
* ğŸ” Local Graph RAG evidence (entity-based)
* ğŸŒ Global Graph RAG evidence (community-based)

---

## ğŸ§  System Architecture Overview

1. **Text Processing**

   * Page-wise extraction and semantic chunking

2. **Embedding Generation**

   * Sentence-transformer embeddings for chunks and summaries

3. **Knowledge Graph Construction**

   * Nodes: entities
   * Edges: relations + co-occurrence

4. **Community Detection**

   * Thematic clustering of graph nodes

5. **Dual Retrieval (SemRAG)**

   * Local Graph RAG â†’ precise evidence
   * Global Graph RAG â†’ thematic context

6. **Answer Generation**

   * Local LLM with strict prompt grounding

---

## ğŸ¯ Design Goals

* Prevent hallucinations
* Preserve author intent
* Enable explainable retrieval
* Support academic & exam-style questions
* Demonstrate SemRAG principles clearly

---

## ğŸ§ª Notes

* All preprocessing outputs are precomputed and stored in `data/`
* The system can be extended to other books by re-running the pipeline
* No external APIs or cloud services are required

